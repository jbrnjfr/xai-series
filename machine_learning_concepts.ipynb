{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbrnjfr/xai-series/blob/master/machine_learning_concepts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Numerical Data for ML\n",
        "## Data Splitting\n",
        "Splitting the dataset into training and testing subsets is crucial for evaluating how well a machine learning model generalizes to unseen data.\n",
        "\n",
        "- Training Set: Used to train the model and learn the patterns.\n",
        "- Testing Set: Used to evaluate the model's performance.\n",
        "- Typical split ratio: 80% training and 20% testing.\n",
        "\n",
        "### Why is it important?\n",
        "- Prevents overfitting by testing the model on unseen data.\n",
        "Ensures fair evaluation metrics.\n"
      ],
      "metadata": {
        "id": "xLPR6GKDC1LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "2Ov9mYl9C4RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Load the dataset\n",
        "file_path = '/content/heart_failure_clinical_records.csv'\n",
        "diabetes_data = pd.read_csv(file_path)\n",
        "diabetes_data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:28:49.444936Z",
          "iopub.execute_input": "2025-03-19T03:28:49.445568Z",
          "iopub.status.idle": "2025-03-19T03:28:49.480547Z",
          "shell.execute_reply.started": "2025-03-19T03:28:49.445536Z",
          "shell.execute_reply": "2025-03-19T03:28:49.479039Z"
        },
        "id": "j3_G4gxyC1LV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "a7de66d6-d7e5-4c2e-b80e-1cea4eea751d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0c5b06ced4a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/heart_failure_clinical_records.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdiabetes_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdiabetes_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target variable\n",
        "features = diabetes_data.drop('DEATH_EVENT', axis = 1)  # independent variables\n",
        "target = diabetes_data['DEATH_EVENT']                   # dependent variables\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "V5chO2tmLV2x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "88503e02-e6e1-42ab-b8a1-d686849a863f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'diabetes_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f9a09c7fb550>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Select features and target variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabetes_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DEATH_EVENT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# independent variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabetes_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEATH_EVENT'\u001b[0m\u001b[0;34m]\u001b[0m                   \u001b[0;31m# dependent variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the dataset into training and testing sets (80% train, 20% test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'diabetes_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "Normalization scales the data so all features contribute equally to the model. Algorithms like K-Means, SVM, and Logistic Regression perform better with normalized data.\n",
        "\n",
        "- Standardization: Scales data to a mean of 0 and standard deviation of 1.\n",
        "- Min-Max Scaling: Scales data to a range [0, 1].\n",
        "\n",
        "### Why is it important?\n",
        "- Prevents features with large ranges (e.g., age vs income) from dominating the learning process.\n",
        "- Improves algorithm convergence."
      ],
      "metadata": {
        "id": "krUxNUtmC1LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:28:52.297981Z",
          "iopub.execute_input": "2025-03-19T03:28:52.298422Z",
          "iopub.status.idle": "2025-03-19T03:28:52.324619Z",
          "shell.execute_reply.started": "2025-03-19T03:28:52.298394Z",
          "shell.execute_reply": "2025-03-19T03:28:52.323087Z"
        },
        "id": "8LCsS0F_C1LW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection\n",
        "Feature selection involves selecting the most relevant features for training a model, which helps improve accuracy and reduces computational complexity.\n",
        "\n",
        "### Why Use ANOVA F-Score for Feature Selection?\n",
        "ANOVA (Analysis of Variance) F-score is used in feature selection because it measures how much a feature contributes to differentiating between classes in a classification problem.\n",
        "1. Measures Discriminative Power\n",
        "The F-score evaluates the variance between groups (different class labels) relative to the variance within each group.\n",
        "A higher F-score means the feature is more useful in distinguishing between classes.\n",
        "2. Works Well for Continuous Data\n",
        "ANOVA F-test is specifically designed for numerical (continuous) data.\n",
        "It is widely used in applications like medical diagnostics (e.g., selecting Glucose and BMI for diabetes prediction) and financial modeling.\n",
        "3. Efficient and Scalable\n",
        "It is a simple and fast statistical test compared to complex methods like Recursive Feature Elimination (RFE).\n",
        "Works well with large datasets where computational efficiency is essential.\n",
        "4. Assesses Individual Feature Importance\n",
        "Unlike model-based approaches, ANOVA F-score ranks features based on their individual impact, making it easy to interpret.\n",
        "\n",
        "### Limitations\n",
        "- Assumes features are normally distributed and independent.\n",
        "- Cannot capture interactions between features.\n",
        "\n",
        "### When to Use ANOVA F-score?\n",
        "- When dealing with classification problems where features are continuous and labels are categorical.\n",
        "- When you need a quick and effective feature ranking method before training models.\n",
        "\n",
        "### Methods:\n",
        "- Manual Selection: Based on domain knowledge (e.g., selecting Glucose and BMI for diabetes prediction).\n",
        "- Statistical Methods:\n",
        "    - ANOVA F-test: Measures variance between groups.\n",
        "    - Recursive Feature Elimination (RFE): Eliminates the least important features iteratively.\n",
        "\n",
        "### How It Works\n",
        "- SelectKBest:\n",
        "    - Selects the top ùëò features based on statistical tests. Here, f_classif is used to compute the ANOVA F-scores.\n",
        "- Results:\n",
        "    - The ANOVA F-scores for each feature are printed.\n",
        "    - The top ùëò features are identified and displayed."
      ],
      "metadata": {
        "id": "3Kl52Sz8C1LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Perform ANOVA to select top k features\n",
        "k = 4  # Select top 3 features\n",
        "anova_selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X_train_selected = anova_selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = anova_selector.transform(X_test_scaled)\n",
        "\n",
        "# Get ANOVA F-scores for all features\n",
        "anova_scores = anova_selector.scores_\n",
        "\n",
        "# Map feature names to their ANOVA F-scores\n",
        "feature_names = features.columns\n",
        "anova_results = dict(zip(feature_names, anova_scores))\n",
        "\n",
        "# Display ANOVA results\n",
        "print(\"ANOVA F-scores for Features:\")\n",
        "for feature, score in anova_results.items():\n",
        "    print(f\"{feature}: {score:.2f}\")\n",
        "\n",
        "# Selected feature indices and names\n",
        "selected_feature_indices = anova_selector.get_support(indices=True)\n",
        "selected_features = feature_names[selected_feature_indices]\n",
        "print(\"\\nSelected Features (Top k):\", list(selected_features))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:29:11.381809Z",
          "iopub.execute_input": "2025-03-19T03:29:11.382289Z",
          "iopub.status.idle": "2025-03-19T03:29:11.688846Z",
          "shell.execute_reply.started": "2025-03-19T03:29:11.382255Z",
          "shell.execute_reply": "2025-03-19T03:29:11.687672Z"
        },
        "id": "0yZg6eUWC1LX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa360d73-e952-44f4-bc4d-66f6523ecd5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOVA F-scores for Features:\n",
            "age: 269.80\n",
            "anaemia: 10.38\n",
            "creatinine_phosphokinase: 33.49\n",
            "diabetes: 1.46\n",
            "ejection_fraction: 383.55\n",
            "high_blood_pressure: 40.74\n",
            "platelets: 1.70\n",
            "serum_creatinine: 422.30\n",
            "serum_sodium: 230.05\n",
            "sex: 17.20\n",
            "smoking: 0.73\n",
            "time: 1559.81\n",
            "\n",
            "Selected Features (Top k): ['age', 'ejection_fraction', 'serum_creatinine', 'time']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "\n",
        "### Definition:\n",
        "- Linear regression is a supervised learning algorithm used to predict a continuous outcome (numerical target). It assumes a linear relationship between the independent variable(s) (ùëã) and the dependent variable (ùë¶).\n",
        "\n",
        "### Key Concept:\n",
        "- The relationship is modeled using the equation of a straight line (recall back in highschool y=mx+b):\n",
        "- y=b0‚Äã+b1‚Äãx1‚Äã+b2‚Äãx2‚Äã+‚ãØ+bn‚Äãxn‚Äã\n",
        "    - y is the predicted outcome.\n",
        "    - b0 is the intercept (the value of ùë¶ when ùë•=0).\n",
        "    - ùëè1,ùëè2,‚Ä¶,ùëèùëõ are the coefficients (slopes of the predictors).\n",
        "    - ùë•1,ùë•2,‚Ä¶,ùë•ùëõ are the independent variables (predictors).\n",
        "\n",
        "### Metrics that we can use to evaluate linear regression models:\n",
        "- Mean Squared Error (MSE):\n",
        "    - Measures the average squared difference between actual and predicted values.\n",
        "    - Formula: ![image.png](attachment:a3fd936d-6b44-48d0-b756-627051eee49f.png)\n",
        "    - Lower values indicate better performance.\n",
        "\n",
        "\n",
        "- Root Mean Squared Error (RMSE):\n",
        "    - Square root of MSE; gives error in the same units as the target variable.\n",
        "    - Formula: ![image.png](attachment:715e0ad6-5133-4c76-9e48-bcade9fd6310.png)\n",
        "- R-Squared (ùëÖ^2):\n",
        "    - Measures the proportion of variance in the target variable explained by the model.\n",
        "    - Formula:![image.png](attachment:8fb8c7c0-6de5-461e-b812-182b7b98f104.png)\n",
        "    - Ranges from 0 to 1; closer to 1 indicates a better fit.\n",
        "\n",
        "\n",
        "### Example Workflow:\n",
        "- Data Collection: Collect a dataset with both predictor variables (ùëã) and a continuous target (ùë¶).\n",
        "- Preprocessing: Handle missing values, scale the data, and split it into training and testing datasets.\n",
        "- Model Training: Use libraries like sklearn to fit a regression model.\n",
        "- Evaluation: Use metrics like MSE, RMSE or ùëÖ^2(coefficient of determination) to assess model performance."
      ],
      "metadata": {
        "id": "UXWgTIhBC1LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression Example:\n",
        "If you wanted to predict blood pressure based on features like glucose level or BMI, you would use linear regression."
      ],
      "metadata": {
        "id": "Btqf5VPxC1LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define features and target\n",
        "X = features[['Glucose', 'BMI', 'Age']]\n",
        "y = target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = lin_reg.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Linear Regression Evaluation:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R-Squared (R¬≤): {r2:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:29:16.802702Z",
          "iopub.execute_input": "2025-03-19T03:29:16.803147Z",
          "iopub.status.idle": "2025-03-19T03:29:16.875233Z",
          "shell.execute_reply.started": "2025-03-19T03:29:16.803117Z",
          "shell.execute_reply": "2025-03-19T03:29:16.873862Z"
        },
        "id": "np9570ZPC1LX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "945509a5-163c-4fe4-c956-60307a431ddd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-02a62814841e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Glucose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BMI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "### Definition:\n",
        "- Logistic regression is a supervised learning algorithm used for binary classification tasks (or multi-class classification with extensions). It estimates the probability that a given input belongs to a particular category.\n",
        "\n",
        "### Key Concept:\n",
        "- Instead of modeling the data with a straight line, logistic regression uses the sigmoid function to squeeze outputs into the range [0, 1]:\n",
        "- ![image.png](attachment:32491247-2fee-409c-ac8b-6289562ec6d1.png)\n",
        "    - P(y=1‚à£x) is the probability of the positive class (1).\n",
        "    - The equation inside the sigmoid function is a linear combination of features.\n",
        "- The threshold (default is 0.5) determines classification:\n",
        "    - If ùëÉ‚â•0.5, classify as 1 (positive class).\n",
        "    - If ùëÉ<0.5, classify as 0 (negative class).\n",
        "\n",
        "### Example Workflow:\n",
        "- Data Collection: Collect labeled data with categorical outcomes.\n",
        "- Preprocessing: Normalize predictors and split the dataset.\n",
        "- Model Training: Use sklearn to fit the logistic regression model.\n",
        "- Evaluation: Evaluate the model using metrics like accuracy, precision, recall, and the F1-score.\n",
        "\n",
        "### Model Evaluation for Classification (Logistic Regression):\n",
        "\n",
        "Logistic Regression is used for binary classification problems. The following metrics are commonly used:\n",
        "\n",
        "#### Confusion Matrix:\n",
        "A matrix summarizing the counts of:\n",
        "- True Positives (TP): Correctly predicted positives.\n",
        "- True Negatives (TN): Correctly predicted negatives.\n",
        "- False Positives (FP): Incorrectly predicted positives.\n",
        "- False Negatives (FN): Incorrectly predicted negatives.\n",
        "![image.png](attachment:a01f100c-b2a7-451d-aa82-45711e27e992.png)\n",
        "\n",
        "#### Accuracy:\n",
        "Measures the proportion of correct predictions.\n",
        "- Formula: ![image.png](attachment:ce03c4bf-ed45-4916-b8d5-8370356b4e01.png)\n",
        "\n",
        "#### Precision:\n",
        "Proportion of positive predictions that are correct.\n",
        "- Formula: ![image.png](attachment:a30f45c9-67d9-44ad-8d95-4fe3028bc873.png)\n",
        "- When to prioritize? Prioritize precision when misdiagnosing a healthy patient as diabetic (FP) is worse than missing a diabetic patient (FN).\n",
        "    - If incorrectly diagnosing a healthy person as diabetic leads to unnecessary stress, lifestyle changes, or expensive medical treatments.\n",
        "    - Example: If the treatment is expensive or has serious side effects, you want to be sure that a positive diagnosis is correct.\n",
        "    - Scenario: A high precision model will ensure that when the model says \"diabetic,\" it is highly likely to be correct.\n",
        "\n",
        "#### Recall (Sensitivity):\n",
        "Proportion of actual positives that are correctly predicted.\n",
        "- Formula: ![image.png](attachment:6076fa21-e02b-40aa-8dff-bbb26823a0ac.png)\n",
        "- When to prioritize? Prioritize recall when missing a diabetic patient (FN) is worse than a false alarm (FP).\n",
        "    - If missing a diabetic patient (false negative) means they won‚Äôt get treatment, leading to serious health complications.\n",
        "    - Example: If early diagnosis can prevent severe diabetes-related complications, recall is crucial.\n",
        "    - Scenario: A high recall model ensures that most diabetic patients are detected, even if it means some healthy people are mistakenly classified as diabetic.\n",
        "\n",
        "#### F1-Score:\n",
        "Harmonic mean of precision and recall.\n",
        "- Formula: ![image.png](attachment:9f884c10-7f00-4aaa-936b-3c5d861b87f8.png)\n",
        "- When to prioritize?\n",
        "    - If the dataset is imbalanced (e.g., 90% non-diabetic, 10% diabetic), a model might predict \"non-diabetic\" most of the time and still get high accuracy. But in reality, it fails to detect diabetes.\n",
        "    - Example: In a screening test for a large population, a balance between precision and recall ensures both minimizing false negatives (missed cases) and false positives (unnecessary medical tests).\n",
        "    - Scenario: If both precision and recall matter, then F1-score is the best choice."
      ],
      "metadata": {
        "id": "u3i3aHP_C1LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Differences from Linear Regression:\n",
        "![image.png](attachment:608e079c-ac8f-44d2-a7b5-d77816ff9bb1.png)"
      ],
      "metadata": {
        "id": "wdnsFT7nC1LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as sm\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Define features and target\n",
        "X = features[['Glucose', 'BMI', 'Age']]\n",
        "y = target  # 0 = No diabetes, 1 = Diabetes\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = log_reg.predict(X_test)\n",
        "conf_matrix = sm.confusion_matrix(y_test, y_pred)\n",
        "accuracy = sm.accuracy_score(y_test, y_pred)\n",
        "precision = sm.precision_score(y_test, y_pred)\n",
        "recall = sm.recall_score(y_test, y_pred)\n",
        "f1 = sm.f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(conf_matrix, annot = True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(sm.classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:29:55.178255Z",
          "iopub.execute_input": "2025-03-19T03:29:55.178731Z",
          "iopub.status.idle": "2025-03-19T03:29:55.454036Z",
          "shell.execute_reply.started": "2025-03-19T03:29:55.178699Z",
          "shell.execute_reply": "2025-03-19T03:29:55.452827Z"
        },
        "id": "pWvyiYgkC1LY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "e951f3ba-e1c0-40d5-bba0-9e0a81198626"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-41e350bdd8e1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Glucose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'BMI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m  \u001b[0;31m# 0 = No diabetes, 1 = Diabetes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Example:\n",
        "If you wanted to predict whether someone has diabetes (Outcome column: 0 = No, 1 = Yes), you'd use logistic regression."
      ],
      "metadata": {
        "id": "cypzmesUC1LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Cross-Validation?\n",
        "- Cross-validation (CV) is a resampling technique used to evaluate a machine learning model‚Äôs performance by splitting the dataset into multiple subsets (folds).\n",
        "- Instead of training and testing on a single split, cross-validation rotates the training and testing process across different partitions, leading to a more generalized performance estimate.\n",
        "- Note: For imbalanced datasets, we use Stratified K-Fold Cross-Validation, which ensures each fold has the same proportion of classes as the original dataset."
      ],
      "metadata": {
        "id": "JILn4Ve3C1LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(LogisticRegression(), X, y, cv=5)\n",
        "scores = scores.mean()\n",
        "print(scores)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:31:48.323585Z",
          "iopub.execute_input": "2025-03-19T03:31:48.324021Z",
          "iopub.status.idle": "2025-03-19T03:31:48.422304Z",
          "shell.execute_reply.started": "2025-03-19T03:31:48.323995Z",
          "shell.execute_reply": "2025-03-19T03:31:48.420874Z"
        },
        "id": "b6ttq_WCC1LY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "- Linear Regression is suitable for problems where the target variable is continuous, such as predicting prices or measurements.\n",
        "- Logistic Regression is ideal for binary classification problems, where the outcome is categorical, such as predicting yes/no or true/false scenarios.\n",
        "- Both models have specific preprocessing needs (e.g., scaling for logistic regression), and their evaluation metrics differ based on the problem type.\n"
      ],
      "metadata": {
        "id": "2Vq2wmrLC1LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree\n",
        "A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. It is a tree-like model of decisions, where:\n",
        "- Internal nodes represent a test on an attribute (e.g., \"Is Glucose > 120?\").\n",
        "- Branches represent possible outcomes of the test.\n",
        "- Leaf nodes represent the final decision/classification.\n",
        "\n",
        "### How Decision Trees Work\n",
        "1. Start at the root node (the entire dataset).\n",
        "2. Select the best feature to split the data, using a criterion like:\n",
        "    - Gini Impurity (default in classification): Measures how often a randomly chosen element would be incorrectly classified.\n",
        "    - Entropy (Information Gain): Measures the uncertainty in a dataset.\n",
        "    - Mean Squared Error (MSE) (for regression): Measures variance reduction.\n",
        "3. Repeat the process recursively until:\n",
        "    - A stopping criterion is met (e.g., max depth reached).\n",
        "    - The data is fully classified.\n",
        "\n",
        "### Key Hyperparameters in Decision Tree Classifier\n",
        "1. criterion (Splitting Measure)\n",
        "    - Controls how the decision tree chooses features for splitting.\n",
        "    - \"gini\" ‚Üí Uses Gini Impurity (default). Measures how impure a node is (lower values are better). Faster and often works well for general cases.\n",
        "    - \"entropy\" ‚Üí Uses Entropy (Information Gain). Measures information gain (higher values are better). Used if dataset has categorical variables and needs interpretability.\n",
        "2. max_depth (Controls Tree Depth)\n",
        "    - Any integer value (default = None, meaning tree grows fully).\n",
        "    - Limits how deep the tree grows.\n",
        "    - A deep tree captures more details but may overfit.\n",
        "    - A shallow tree prevents overfitting but may be too simple.\n",
        "    - Large datasets ‚Üí Use a limited depth (e.g., max_depth=5).\n",
        "    - Small datasets ‚Üí Allow deeper trees (default None).\n",
        "\n",
        "### Advantages of Decision Trees\n",
        "- Easy to interpret and visualize.\n",
        "- Handles both numerical & categorical data.\n",
        "- No need for feature scaling.\n",
        "- Captures non-linear relationships well.\n",
        "\n",
        "### Disadvantages of Decision Trees\n",
        "- Prone to overfitting (solved using pruning or limiting depth).\n",
        "- Sensitive to noisy data (small changes can alter structure significantly).\n",
        "- Not ideal for very large datasets (other methods like Random Forest perform better)."
      ],
      "metadata": {
        "id": "d3mV4oGWC1LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "# Create Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)\n",
        "\n",
        "# Perform 5-Fold Cross-Validation directly on X and y\n",
        "cv_scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "\n",
        "# Define stratified 5-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform Cross-Validation for Different Metrics\n",
        "cv_accuracy = cross_val_score(clf, X, y, cv=cv, scoring='accuracy')\n",
        "cv_precision = cross_val_score(clf, X, y, cv=cv, scoring='precision')\n",
        "cv_recall = cross_val_score(clf, X, y, cv=cv, scoring='recall')\n",
        "cv_f1 = cross_val_score(clf, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "# Perform cross-validation predictions to get confusion matrix\n",
        "y_pred_cv = cross_val_predict(clf, X, y)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y, y_pred_cv)\n",
        "\n",
        "# Plot the Decision Tree\n",
        "clf.fit(X, y)  # Train the model\n",
        "plt.figure(figsize=(12, 8))\n",
        "plot_tree(clf)\n",
        "plt.title(\"Decision Tree for Diabetes Prediction\")\n",
        "plt.show()\n",
        "\n",
        "# Print Cross-Validation Results\n",
        "print(f\"Mean CV Accuracy: {np.mean(cv_accuracy):.4f}\")\n",
        "print(f\"Mean CV Precision: {np.mean(cv_precision):.4f}\")\n",
        "print(f\"Mean CV Recall: {np.mean(cv_recall):.4f}\")\n",
        "print(f\"Mean CV F1-Score: {np.mean(cv_f1):.4f}\")\n",
        "\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"\\nConfusion Matrix (Aggregated from CV Predictions):\")\n",
        "print(conf_matrix)\n",
        "\n",
        "\n",
        "# Display full classification report\n",
        "print(\"\\nClassification Report (Aggregated from CV Predictions):\")\n",
        "print(classification_report(y, y_pred_cv))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:44:23.784489Z",
          "iopub.execute_input": "2025-03-19T03:44:23.784955Z",
          "iopub.status.idle": "2025-03-19T03:44:26.065568Z",
          "shell.execute_reply.started": "2025-03-19T03:44:23.784921Z",
          "shell.execute_reply": "2025-03-19T03:44:26.064238Z"
        },
        "id": "OPbnDG9rC1LZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "- Random Forest is an ensemble learning method that combines multiple Decision Trees to improve classification or regression performance. It is widely used for tasks like disease prediction, fraud detection, stock price forecasting, and customer segmentation.\n",
        "\n",
        "### How Does Random Forest Work?\n",
        "1. Bootstrap Sampling (Bagging):\n",
        "- The dataset is randomly sampled with replacement (bootstrapping) to create multiple subsets.\n",
        "- Each subset is used to train a separate Decision Tree.\n",
        "2. Feature Randomization:\n",
        "- At each split, only a random subset of features is considered instead of all features.\n",
        "- This introduces diversity in the trees and reduces correlation between them.\n",
        "3. Aggregation (Voting or Averaging):\n",
        "- For classification: Majority Voting is used (most common class label among trees).\n",
        "- For regression: Averaging of predictions is used.\n",
        "\n",
        "### Hyperparameters of Random Forest\n",
        "1. n_estimators (Number of Trees)\n",
        "- Controls how many trees the model builds (default = 100).\n",
        "- Higher values improve performance but increase computation.\n",
        "2. max_features (Number of Features Considered)\n",
        "- Controls how many features are randomly chosen at each split.\n",
        "- Options:\n",
        "    - \"auto\" ‚Üí Uses sqrt(features) for classification (default).\n",
        "    - \"log2\" ‚Üí Uses log2(features).\n",
        "    - None ‚Üí Uses all features.\n",
        "3. the hyperparameters from Decision Trees\n",
        "\n",
        "### Why Use Random Forest?\n",
        "‚úÖ Reduces Overfitting: Unlike Decision Trees, it generalizes better.\n",
        "\n",
        "‚úÖ Handles Missing Data & Noisy Data: More robust to anomalies.\n",
        "\n",
        "‚úÖ Works with Both Classification & Regression Tasks.\n",
        "\n",
        "‚úÖ Feature Importance Ranking: Helps identify key variables in prediction.\n",
        "\n",
        "üî¥ Downsides:\n",
        "\n",
        "‚ùå Computationally expensive (many trees).\n",
        "\n",
        "‚ùå Harder to interpret than a single Decision Tree.\n"
      ],
      "metadata": {
        "id": "3M6fgYYqC1LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Select relevant features and target variable\n",
        "X = features[['Glucose', 'BMI', 'Age']]\n",
        "y = target  # 1 = Diabetic, 0 = Non-Diabetic\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Define Stratified 5-Fold Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform Cross-Validation for Different Metrics\n",
        "cv_accuracy = cross_val_score(rf, X, y, cv=cv, scoring='accuracy')\n",
        "cv_precision = cross_val_score(rf, X, y, cv=cv, scoring='precision')\n",
        "cv_recall = cross_val_score(rf, X, y, cv=cv, scoring='recall')\n",
        "cv_f1 = cross_val_score(rf, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "# Perform cross-validation predictions to get confusion matrix\n",
        "y_pred_cv = cross_val_predict(rf, X, y, cv=cv)\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y, y_pred_cv)\n",
        "\n",
        "# Print Cross-Validation Results\n",
        "print(f\"Mean CV Accuracy: {np.mean(cv_accuracy):.4f}\")\n",
        "print(f\"Mean CV Precision: {np.mean(cv_precision):.4f}\")\n",
        "print(f\"Mean CV Recall: {np.mean(cv_recall):.4f}\")\n",
        "print(f\"Mean CV F1-Score: {np.mean(cv_f1):.4f}\")\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"\\nConfusion Matrix (Aggregated from CV Predictions):\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Display full classification report\n",
        "print(\"\\nClassification Report (Aggregated from CV Predictions):\")\n",
        "print(classification_report(y, y_pred_cv))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:50:16.691731Z",
          "iopub.execute_input": "2025-03-19T03:50:16.692333Z",
          "iopub.status.idle": "2025-03-19T03:50:27.817774Z",
          "shell.execute_reply.started": "2025-03-19T03:50:16.692275Z",
          "shell.execute_reply": "2025-03-19T03:50:27.816572Z"
        },
        "id": "WxqjfLDDC1LZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Takeaways\n",
        "- Cross-validation ensures better generalization compared to a single train-test split.\n",
        "- Random Forest handles overfitting well by combining multiple decision trees.\n",
        "- Aggregating results across folds provides a more accurate picture of model performance.\n"
      ],
      "metadata": {
        "id": "k3PlZVdYC1LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing results from Logistic Regression, Decision Tree, and Random Forest Tree, Random Frest Tree gives the best results. So this is the best model so far."
      ],
      "metadata": {
        "id": "CMM8weCpC1LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning: Overview of K-Means and Hierarchical Clustering\n",
        "- Unsupervised learning, particularly clustering, is widely used in the medical field for tasks like patient segmentation, disease classification, and treatment personalization. Let's explore two popular clustering techniques: K-Means and Hierarchical Clustering."
      ],
      "metadata": {
        "id": "g3iyuGZMC1LZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Clustering\n",
        "### Definition:\n",
        "- K-Means clustering partitions a dataset into ùëò groups (clusters) by minimizing the variance within each cluster. It is a centroid-based algorithm that works iteratively to group data points based on their similarity.\n",
        "\n",
        "#### Key Idea of K-Means\n",
        "- It tries to group similar data points together while keeping them as far apart from other groups as possible.\n",
        "- It does this by minimizing the intra-cluster distance (distance between points within a cluster) and maximizing the inter-cluster distance (distance between different clusters).\n",
        "\n",
        "### Steps in K-Means:\n",
        "1. Select ùëò, the number of clusters.\n",
        "2. Randomly initialize ùëò centroids (points representing the cluster center).\n",
        "3. Assign each data point to the nearest centroid.\n",
        "4. Update the centroids based on the mean of all points in the cluster.\n",
        "5. Repeat steps 3‚Äì4 until centroids no longer change or a maximum number of iterations is reached.\n",
        "\n",
        "### Key Hyperparameters of K-Means\n",
        "1. Number of Clusters (K)\n",
        "- The number of clusters to divide the dataset into.\n",
        "- How to choose K?\n",
        "    - Elbow Method: Plot the inertia (within-cluster sum of squares) for different K values and find the \"elbow\" point where adding more clusters doesn‚Äôt significantly reduce inertia.\n",
        "\n",
        "2. n_init (Number of Initializations)\n",
        "- K-Means is sensitive to the initial placement of centroids. Running the algorithm multiple times with different initializations can improve results.\n",
        "- Recommended value: n_init = auto.\n",
        "\n",
        "### Strengths:\n",
        "- Efficient and scalable for large datasets.\n",
        "- Easy to implement.\n",
        "- Can handle different types of clustering problems.\n",
        "\n",
        "### Weaknesses:\n",
        "- Sensitive to the initial choice of ùëò and centroid placement.\n",
        "- May struggle with clusters of varying sizes or non-spherical shapes.\n",
        "- Requires pre-defining K (difficult when the number of clusters is unknown).\n"
      ],
      "metadata": {
        "id": "Fba4W_b_C1LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select relevant features for clustering\n",
        "X = features[['Glucose', 'BMI']]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(X)\n",
        "\n",
        "# Calculate WCSS (Within-Cluster Sum of Squares) for different K values\n",
        "wcss = []\n",
        "K_range = range(1, 11)  # Trying K from 1 to 10\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "    kmeans.fit(scaled_features)\n",
        "    wcss.append(kmeans.inertia_)  # Inertia is the sum of squared distances to nearest centroid\n",
        "\n",
        "# Plot the Elbow Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, wcss, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.xticks(K_range)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:32:02.156293Z",
          "iopub.execute_input": "2025-03-19T03:32:02.156731Z",
          "iopub.status.idle": "2025-03-19T03:32:02.886487Z",
          "shell.execute_reply.started": "2025-03-19T03:32:02.156700Z",
          "shell.execute_reply": "2025-03-19T03:32:02.884698Z"
        },
        "id": "-w8t6j4FC1LZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
        "diabetes_data['KMeans_Cluster'] = kmeans.fit_predict(scaled_features)\n",
        "\n",
        "# Visualize K-Means Clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X['Glucose'], X['BMI'], c=diabetes_data['KMeans_Cluster'], cmap='viridis', alpha=0.7)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0] * scaler.scale_[0] + scaler.mean_[0],\n",
        "            kmeans.cluster_centers_[:, 1] * scaler.scale_[1] + scaler.mean_[1],\n",
        "            s=200, c='red', label='Centroids', edgecolor='black')\n",
        "plt.title('K-Means Clustering (Glucose vs BMI)')\n",
        "plt.xlabel('Glucose')\n",
        "plt.ylabel('BMI')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T03:32:04.897398Z",
          "iopub.execute_input": "2025-03-19T03:32:04.897847Z",
          "iopub.status.idle": "2025-03-19T03:32:05.288334Z",
          "shell.execute_reply.started": "2025-03-19T03:32:04.897818Z",
          "shell.execute_reply": "2025-03-19T03:32:05.286998Z"
        },
        "id": "g_KQkEG8C1LZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means Clustering Results:\n",
        "- Patients are grouped into three clusters based on their Glucose and BMI levels.\n",
        "- Centroids represent the central point of each cluster.\n",
        "- Clusters may correspond to low, medium, and high-risk groups for diabetes based on glucose levels and BMI."
      ],
      "metadata": {
        "id": "tXfeaq8AC1La"
      }
    }
  ]
}